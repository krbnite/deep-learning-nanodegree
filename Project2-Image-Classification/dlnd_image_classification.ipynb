{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## KrbNote\n",
    "To play around with the raw data, import helper in iPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47fb34dd68>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = np.array(x)\n",
    "    return x/255.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "lab_to_hot = {\n",
    "    0: [1]+list(np.zeros(9,dtype='int32')),\n",
    "    1: [0,1]+list(np.zeros(8,dtype='int32')),\n",
    "    2: [0,0,1]+list(np.zeros(7,dtype='int32')),\n",
    "    3: [0,0,0,1]+list(np.zeros(6,dtype='int32')),\n",
    "    4: [0,0,0,0,1]+list(np.zeros(5,dtype='int32')),\n",
    "    5: list(np.zeros(5,dtype='int32'))+[1,0,0,0,0],\n",
    "    6: list(np.zeros(6,dtype='int32'))+[1,0,0,0],\n",
    "    7: list(np.zeros(7,dtype='int32'))+[1,0,0],\n",
    "    8: list(np.zeros(8,dtype='int32'))+[1,0],\n",
    "    9: list(np.zeros(9,dtype='int32'))+[1]\n",
    "    }\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    output=[lab_to_hot[label] for label in x]\n",
    "    return np.array(output)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    img=image_shape\n",
    "    return tf.placeholder(tf.float32, shape=(None,img[0],img[1],img[2]), name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None,n_classes), name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, padding='SAME',\n",
    "                  activation=None, keep_prob=None): \n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    filt_h = conv_ksize[0];    filt_w = conv_ksize[1]\n",
    "    fstr_h = conv_strides[0];  fstr_w = conv_strides[1]\n",
    "    pool_h = pool_ksize[0];    pool_w = pool_ksize[1]\n",
    "    pstr_h = pool_strides[0];  pstr_w = pool_strides[1]\n",
    "    x_h = x_tensor.get_shape()[1].value\n",
    "    x_w = x_tensor.get_shape()[2].value\n",
    "    x_d = x_tensor.get_shape()[3].value\n",
    "    n_features = filt_h * filt_w * x_d\n",
    "    \n",
    "    #=======================================\n",
    "    # Layer Parameters\n",
    "    #=======================================\n",
    "    weights = tf.Variable(tf.truncated_normal([filt_h,filt_w,x_d,conv_num_outputs],\n",
    "                                               mean=0, stddev=np.sqrt(2.0/n_features) ))#0.1\n",
    "    #bias = tf.Variable(tf.truncated_normal([conv_num_outputs], mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "            \n",
    "    #=======================================\n",
    "    # Convolution Layer\n",
    "    #=======================================\n",
    "    conv_layer = tf.nn.conv2d( x_tensor, weights, strides=[1,fstr_h,fstr_w,1], padding=padding )\n",
    "    conv_layer = tf.nn.bias_add( conv_layer, bias )\n",
    "    \n",
    "    #=======================================\n",
    "    # Activation Function\n",
    "    #=======================================\n",
    "    if activation == 'relu':\n",
    "        conv_layer = tf.nn.relu(conv_layer)\n",
    "    elif activation == 'sigmoid':\n",
    "        conv_layer = tf.nn.sigmoid(conv_layer)\n",
    "    elif activation == 'tanh':\n",
    "        conv_layer = tf.nn.tanh(conv_layer)\n",
    "    elif activation == 'softplus':\n",
    "        conv_layer = tf.nn.softplus(conv_layer)\n",
    "    \n",
    "    #=======================================\n",
    "    # Max Pooling Option\n",
    "    #=======================================\n",
    "    conv_layer = tf.nn.max_pool( conv_layer, ksize=[1,pool_h,pool_w,1], \n",
    "                                strides=[1,pstr_h,pstr_w,1], padding='SAME' )\n",
    "    \n",
    "    #=======================================\n",
    "    # Drop Out Option\n",
    "    #=======================================\n",
    "    if keep_prob != None:\n",
    "        conv_layer = tf.nn.dropout(conv_layer, keep_prob)\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    img_h = x_tensor.get_shape()[1].value\n",
    "    img_w = x_tensor.get_shape()[2].value\n",
    "    img_d = x_tensor.get_shape()[3].value\n",
    "    return tf.reshape(x_tensor,shape=(-1,img_h*img_w*img_d))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### A Little Extra Something:  Inflate Layer\n",
    "This reverse the flatten() operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inflate(x_tensor, img_h,img_w, img_d):\n",
    "    \"\"\"\n",
    "    Inflate x_tensor to (Batch Size, height, width, depth)\n",
    "    : x_tensor: A tensor of size (Batch Size, fDim), where fDim is the flattened image dimension.\n",
    "    : return: A tensor of size (Batch Size, height, width, depth).\n",
    "    \"\"\"\n",
    "    img_f = x_tensor.get_shape()[1].value\n",
    "    assert img_f == img_h*img_w*img_d\n",
    "    \n",
    "    return tf.reshape(x_tensor,shape=(-1,img_h,img_w,img_d))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs, activation=None, keep_prob=None):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    img_hwd = x_tensor.get_shape()[1].value\n",
    "    n_features = img_hwd\n",
    "    \n",
    "    #=======================================\n",
    "    # Layer Parameters\n",
    "    #=======================================\n",
    "    weight = tf.Variable(tf.truncated_normal([img_hwd, num_outputs], mean=0, stddev=np.sqrt(2.0/n_features)))#0.1\n",
    "    #bias   = tf.Variable(tf.truncated_normal([num_outputs], mean=0, stddev=0.01))\n",
    "    bias   = tf.Variable(tf.zeros([num_outputs]))\n",
    "        \n",
    "    #=======================================\n",
    "    # Fully Connected Layer\n",
    "    #=======================================\n",
    "    fc_layer = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "    #=======================================\n",
    "    # Activation Function\n",
    "    #=======================================\n",
    "    if activation=='relu':\n",
    "        fc_layer = tf.nn.relu(fc_layer)\n",
    "    elif activation=='sigmoid':\n",
    "        fc_layer = tf.nn.sigmoid(fc_layer)\n",
    "    elif activation=='tanh':\n",
    "        fc_layer = tf.nn.tanh(fc_layer)\n",
    "    elif activation=='softplus':\n",
    "        fc_layer = tf.nn.softplus(fc_layer)\n",
    "        \n",
    "    #=======================================\n",
    "    # Post-Layer Drop Out Option\n",
    "    #=======================================\n",
    "    if keep_prob != None:\n",
    "        fc_layer = tf.nn.dropout(fc_layer, keep_prob)\n",
    "       \n",
    "    return fc_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    output_layer = fully_conn(x_tensor,num_outputs)\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. \n",
    "\n",
    "**Note to Self**: the input images are 32x32x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_3/mul:0' shape=(?, 50) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_prob=0.6\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "cl = conv2d_maxpool(x, conv_num_outputs=25, conv_ksize=(5,5), conv_strides=(1,1), \n",
    "         pool_ksize=(2,2), pool_strides=(2,2), activation='relu', keep_prob=keep_prob)\n",
    "cl = conv2d_maxpool(cl, conv_num_outputs=50, conv_ksize=(7,7), conv_strides=(1,1), \n",
    "        pool_ksize=(2,2), pool_strides=(2,2), activation='relu', keep_prob=keep_prob)\n",
    "cl = conv2d_maxpool(cl, conv_num_outputs=100, conv_ksize=(9,9), conv_strides=(1,1), \n",
    "           pool_ksize=(1,1), pool_strides=(1,1), activation='relu', keep_prob=keep_prob)\n",
    "fcl = flatten(cl)\n",
    "fcl = fully_conn(fcl,  50, activation='relu', keep_prob=keep_prob)#, keep_prob=0.8) # Drop Out on this layer is too expensive in epochs\n",
    "\n",
    "fcl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    \n",
    "    # Keep_Prob by Layer Type\n",
    "    # -- change coefficients to experiment\n",
    "    # -- ultimately found not to have huge impact...\n",
    "    conv_kp = (7.0/5.0)*keep_prob\n",
    "    fc_kp = keep_prob  \n",
    "    \n",
    "    \n",
    "    # CL1: (b,32,32,3)-dim Tensor --> (b,,,)\n",
    "    #x=tf.nn.dropout(x,keep_prob)\n",
    "    cl = conv2d_maxpool(x, conv_num_outputs=50, conv_ksize=(5,5), conv_strides=(1,1), \n",
    "           pool_ksize=(2,2), pool_strides=(2,2), activation='relu', keep_prob=conv_kp)\n",
    "    \n",
    "    # CL2: (b,,,)-dim Tensor --> (b,,,)\n",
    "    #cl = tf.nn.dropout(cl,keep_prob)\n",
    "    cl = conv2d_maxpool(cl, conv_num_outputs=100, conv_ksize=(5,5), conv_strides=(1,1), \n",
    "           pool_ksize=(2,2), pool_strides=(2,2), activation='relu', keep_prob=conv_kp)\n",
    "    \n",
    "    # CL3: (b,,,)-dim Tensor --> (b,6,,)\n",
    "    #cl = tf.nn.dropout(cl,keep_prob)\n",
    "    #cl = conv2d_maxpool(cl, conv_num_outputs=1000, conv_ksize=(3,3), conv_strides=(1,1), \n",
    "    #       pool_ksize=(1,1), pool_strides=(1,1), activation='relu', keep_prob=conv_kp)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # (b,,,) -->  (b,)\n",
    "    fcl = flatten(cl)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # (b,,,) -->  (b,)\n",
    "    fcl = fully_conn(fcl,  100, activation='sigmoid', keep_prob = fc_kp)#, keep_prob=0.8) # Drop Out on this layer is too expensive in epochs\n",
    "    \n",
    "    # 2nd Layer generally degrades performance (or at least doesn't seem to add much)......\n",
    "    #fcl = fully_conn(fcl,  30, activation='relu', keep_prob=fc_kp)\n",
    "        \n",
    "    # 3rd FCL totally not necessary\n",
    "    #fcl = fully_conn(fcl,  100, activation='sigmoid', keep_prob=keep_prob)\n",
    "    \n",
    "#400,200 (or no fcl)\n",
    "\n",
    "\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    ol = output(fcl, 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return ol\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### List Comprehensions\n",
    "I implemented the one-hot encoder with a for loop initially.  Didn't matter until I went to test `train_neural_network` below.  Then it mattered big time.  Literally.  Suffered through a half hour before I interrupted the kernel b/c \"this can't be right!\"  Looked through my code and realized with horror I'd made a rookie move: for loop spotted!  I replaced the monstrosity with a list comprehension, then re-ran the cells, stopping here at `train_neural_network`.  With trepidation, my fingers made the next move: &lt;shift&gt;+&lt;enter&gt;...  Before I could look away, the cell completed -- test passed!  Moral of story: Whoa, list comprehensions are tachyonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note to Self**:  The global variables are defined above like so:\n",
    "<center>valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    valid_size = valid_features.shape[0]\n",
    "    loss = session.run( cost, feed_dict={\n",
    "            x: feature_batch, \n",
    "            y: label_batch, \n",
    "            keep_prob: 1.} )\n",
    "    acc  = session.run(accuracy, feed_dict={\n",
    "            x: valid_features,\n",
    "            y: valid_labels, \n",
    "            keep_prob: 1.})\n",
    "\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "            loss,  acc))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 32\n",
    "keep_probability = 0.5\n",
    "# Note: keep_prob is for conv layers, (2/3)*keep_probability for FC Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     1.7816 Validation Accuracy: 0.397400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.6070 Validation Accuracy: 0.418400\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.4018 Validation Accuracy: 0.496200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.2261 Validation Accuracy: 0.520600\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.9604 Validation Accuracy: 0.544000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.9311 Validation Accuracy: 0.546200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.7201 Validation Accuracy: 0.567800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.7331 Validation Accuracy: 0.587600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.6175 Validation Accuracy: 0.591400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.4926 Validation Accuracy: 0.610600\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.4367 Validation Accuracy: 0.602200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.4005 Validation Accuracy: 0.600200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.3051 Validation Accuracy: 0.611400\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.3017 Validation Accuracy: 0.599800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.2205 Validation Accuracy: 0.612400\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.2067 Validation Accuracy: 0.623200\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.2530 Validation Accuracy: 0.612800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.2257 Validation Accuracy: 0.620200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.1616 Validation Accuracy: 0.619200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.2171 Validation Accuracy: 0.624400\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.1512 Validation Accuracy: 0.626400\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.1102 Validation Accuracy: 0.632400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.1232 Validation Accuracy: 0.615600\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.1210 Validation Accuracy: 0.634600\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.1119 Validation Accuracy: 0.630800\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.0580 Validation Accuracy: 0.622800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.0828 Validation Accuracy: 0.630800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.0578 Validation Accuracy: 0.630200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.0678 Validation Accuracy: 0.632800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.0226 Validation Accuracy: 0.626200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     1.7759 Validation Accuracy: 0.395800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.3657 Validation Accuracy: 0.451600\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.1986 Validation Accuracy: 0.495000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.5230 Validation Accuracy: 0.512600\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.4804 Validation Accuracy: 0.552800\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.2308 Validation Accuracy: 0.560200\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     0.8088 Validation Accuracy: 0.572600\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     0.8277 Validation Accuracy: 0.587000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.0635 Validation Accuracy: 0.605400\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.3458 Validation Accuracy: 0.622800\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     0.9441 Validation Accuracy: 0.621800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     0.4681 Validation Accuracy: 0.628200\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.5790 Validation Accuracy: 0.635200\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.6479 Validation Accuracy: 0.644200\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.3630 Validation Accuracy: 0.648200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.8398 Validation Accuracy: 0.648400\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.4474 Validation Accuracy: 0.656600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.6028 Validation Accuracy: 0.649200\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.6437 Validation Accuracy: 0.672800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.1432 Validation Accuracy: 0.675200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.6059 Validation Accuracy: 0.665800\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.5601 Validation Accuracy: 0.679800\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.3533 Validation Accuracy: 0.670000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.5037 Validation Accuracy: 0.672800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.0217 Validation Accuracy: 0.676400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.5421 Validation Accuracy: 0.672200\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.5161 Validation Accuracy: 0.684000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.4519 Validation Accuracy: 0.691600\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.5059 Validation Accuracy: 0.695400\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.7755 Validation Accuracy: 0.685800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.3281 Validation Accuracy: 0.685600\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.4266 Validation Accuracy: 0.688200\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.2936 Validation Accuracy: 0.694000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.5492 Validation Accuracy: 0.696600\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.7400 Validation Accuracy: 0.705600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.6680 Validation Accuracy: 0.690600\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.3825 Validation Accuracy: 0.701200\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.2892 Validation Accuracy: 0.702200\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.6521 Validation Accuracy: 0.691800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.6365 Validation Accuracy: 0.706000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.4112 Validation Accuracy: 0.694200\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.3538 Validation Accuracy: 0.706600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.2677 Validation Accuracy: 0.699200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.4780 Validation Accuracy: 0.712600\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.6532 Validation Accuracy: 0.707200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.2805 Validation Accuracy: 0.690800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.3759 Validation Accuracy: 0.701000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.2478 Validation Accuracy: 0.708400\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.3719 Validation Accuracy: 0.708600\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.5388 Validation Accuracy: 0.712000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.2287 Validation Accuracy: 0.696600\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.3535 Validation Accuracy: 0.693400\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.3044 Validation Accuracy: 0.712400\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.4507 Validation Accuracy: 0.710400\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.5490 Validation Accuracy: 0.709200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.2488 Validation Accuracy: 0.702400\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.3157 Validation Accuracy: 0.715800\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.2361 Validation Accuracy: 0.722400\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.3722 Validation Accuracy: 0.728800\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.5030 Validation Accuracy: 0.714400\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.1535 Validation Accuracy: 0.721400\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.3395 Validation Accuracy: 0.717000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.2227 Validation Accuracy: 0.726200\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.3938 Validation Accuracy: 0.718200\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.4661 Validation Accuracy: 0.723800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.1855 Validation Accuracy: 0.724400\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.2996 Validation Accuracy: 0.729200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.1347 Validation Accuracy: 0.729600\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.2827 Validation Accuracy: 0.715200\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.3248 Validation Accuracy: 0.725400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.2922 Validation Accuracy: 0.710800\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.2854 Validation Accuracy: 0.723800\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.2321 Validation Accuracy: 0.736400\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.4098 Validation Accuracy: 0.725600\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.3742 Validation Accuracy: 0.731800\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.2746 Validation Accuracy: 0.724400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.3343 Validation Accuracy: 0.723200\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.1908 Validation Accuracy: 0.722200\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.1977 Validation Accuracy: 0.729200\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.3408 Validation Accuracy: 0.723400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.1388 Validation Accuracy: 0.717600\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.4509 Validation Accuracy: 0.722200\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.1511 Validation Accuracy: 0.727400\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.2231 Validation Accuracy: 0.710800\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.3478 Validation Accuracy: 0.707000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.2377 Validation Accuracy: 0.723600\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.2869 Validation Accuracy: 0.716200\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.3234 Validation Accuracy: 0.737400\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.1641 Validation Accuracy: 0.729400\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.3927 Validation Accuracy: 0.736800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.1665 Validation Accuracy: 0.724200\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.2117 Validation Accuracy: 0.732000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.2887 Validation Accuracy: 0.725400\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.2441 Validation Accuracy: 0.723400\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.5378 Validation Accuracy: 0.734600\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.1410 Validation Accuracy: 0.727000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.2253 Validation Accuracy: 0.729000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.1777 Validation Accuracy: 0.733000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.2156 Validation Accuracy: 0.733000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.5223 Validation Accuracy: 0.730800\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.1978 Validation Accuracy: 0.717200\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.4535 Validation Accuracy: 0.732800\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.1828 Validation Accuracy: 0.732400\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.2674 Validation Accuracy: 0.734200\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.3772 Validation Accuracy: 0.735600\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.0902 Validation Accuracy: 0.728200\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.3262 Validation Accuracy: 0.734400\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.1549 Validation Accuracy: 0.734000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.2517 Validation Accuracy: 0.736600\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.4642 Validation Accuracy: 0.740200\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.1004 Validation Accuracy: 0.734400\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.1787 Validation Accuracy: 0.724400\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.1308 Validation Accuracy: 0.734400\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.2887 Validation Accuracy: 0.728200\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.4507 Validation Accuracy: 0.739200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.1094 Validation Accuracy: 0.729600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.1932 Validation Accuracy: 0.742400\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.1009 Validation Accuracy: 0.743000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.5836 Validation Accuracy: 0.725800\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.3576 Validation Accuracy: 0.734800\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.2163 Validation Accuracy: 0.735600\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.2028 Validation Accuracy: 0.727200\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.0864 Validation Accuracy: 0.732200\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.2673 Validation Accuracy: 0.736600\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.4013 Validation Accuracy: 0.731400\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.2408 Validation Accuracy: 0.737600\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.1840 Validation Accuracy: 0.726400\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.1009 Validation Accuracy: 0.733400\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.2232 Validation Accuracy: 0.739800\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.3919 Validation Accuracy: 0.734600\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.1284 Validation Accuracy: 0.717000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.1547 Validation Accuracy: 0.734800\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.1171 Validation Accuracy: 0.735200\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.2170 Validation Accuracy: 0.734200\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.5190 Validation Accuracy: 0.735600\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.0995 Validation Accuracy: 0.732400\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.2859 Validation Accuracy: 0.734200\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.1682 Validation Accuracy: 0.741800\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.2640 Validation Accuracy: 0.732400\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.3034 Validation Accuracy: 0.740200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.1018 Validation Accuracy: 0.728400\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.1627 Validation Accuracy: 0.737000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.1890 Validation Accuracy: 0.735600\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.2714 Validation Accuracy: 0.741800\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.2907 Validation Accuracy: 0.732400\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.0847 Validation Accuracy: 0.732200\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.3009 Validation Accuracy: 0.739800\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.2196 Validation Accuracy: 0.728800\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.2178 Validation Accuracy: 0.741600\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.3939 Validation Accuracy: 0.737400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7349241214057508\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc536bf3f28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Notes 2 Self\n",
    "CIFAR-10 has 60k 32x32x3 images.  Each training batch file has 10k images, which lies between 2<sup>13</sup> and 2<sup>14</sup>.\n",
    "\n",
    "### Batch Size\n",
    "Generally seems like the smaller my batch size, the better the validation accuracy...but only up to a point. For example, a batch size of 4 seems to perform terribly (stuck around 0.1), and though a batch size of 8 seem to climb, it does so slowly and with deceleration, eventually stagnating in a suboptimal region of the parameter space.  A batch size of 16 sometimes works pretty well, other times not so much.  Seems like somewhere in the range of 32-64 is an optimal batch size. However, this can make the training time extremely long if the network has too many weights to learn.\n",
    "\n",
    "So I Googled it and came to this [StackExchange](http://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network) w/ this quote:\n",
    "\n",
    "\"The stochastic gradient descent method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, usually 32--512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. There have been some attempts to investigate the cause for this generalization drop in the large-batch regime, however the precise answer for this phenomenon is, hitherto unknown. In this paper, we present ample numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions -- and that sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We also discuss several empirical strategies that help large-batch methods eliminate the generalization gap and conclude with a set of future research ideas and open questions.\"\n",
    "\n",
    "### Sensitivity of Initialization \n",
    "My model was performing terribly for a while, and it wasn't only because my AWS g2.2xlarge instanced hadn't been approved of by Amazon yet.  No matter what I did, my model was as good as a guess (10%) or worse!\n",
    "\n",
    "What the heck could be going on?  Turned out that my parameter initializations were too big.  After setting the stddev in my truncated_normal() expression to 0.1 things began to sky rocket...sort of.  Things actually became frustrating for a while:  though I was doing much better than a guess, it seemed no matter what I tried, my net would get stuck just under 0.5 validation accuracy. I tweaked and re-tweaked my convolutional layers, added more dense layers, removed layers, and even create the \"inflate function\" above so that I could toggle between convolutional and dense layers... But nothing.  Still under 0.5\n",
    "\n",
    "Solution? You guessed it: tweaked my initializations again and made them even smaller (0.01).  Things seemed better: I would occasionally fluctuate above 0.5, but only to drop again to somewhere just a nudge below.  Should I go smaller? Nope: 0.001 seemed to make things worse.  \n",
    "\n",
    "Ultimately, after a lot of reading, I set it to sqrt(2.0/n_features), which seems to be fairly common.  The initializations need to take on slightly different scales depending on the specifics going into the layer. For a convolutional layer, I presume this amounts to (filter area) x (input depth).   \n",
    "\n",
    "For example, for my first convolutional layer I use 5x5 filter over a 3-channel image, which means that the convolutional layer sees 5x5x3=75 incoming features.  This corresponds to stdev = sqrt(2.0/75) = 0.16.  However, if I choose an output depth of 16 on this layer, and a 7x7 filter on my next convolutional layer, then this layer sees an equivalent of 7x7x16=784 incoming features, which suggests an initialization of stedev = sqrt(2.0/784) = 0.05.\n",
    "\n",
    "\n",
    "### Keep Probability\n",
    "Seems that a KP of 0.5 has a low start (~0.1), but asteady, consistent growth in validation accuracy.  In contrast, a KP of 0.75 starts out with a validation accuaracy that is much higher (~0.3) and grows quickly at first, but then slows down quite a bit.  Given 50 epochs, both seem to perform about the same... A KP of 0.25 performs much worse, stagnating at no better than a guess.\n",
    "\n",
    "### Fully Connected Layers:  Wide, Narrow, Few, Many\n",
    "On my intial designs, I found that one really wide FCL will get your pretty far, pretty fast---then stagnate.  The best I could seem to do was a 0.5 validation accuracy.  If you keep it wide and add a second FCL, your net is basically doomed: learning becomes damn near impossible.  However, if you instead use two fairly narrow FCLs, then you get about the same performance as the one really wide FCL... In fact, the more I've experimented the more it seems like you really have to make that first FC Layer after the last conv layer pretty damn narrow; otherwise there are way too many weights to learn.\n",
    "\n",
    "Later on, I realized my biggest bottleneck was poorly designing the convolutional layers.  In fact, it seems a good design at the convolution stage dictates your model performances almost exclusively.  See section investigating whether or not FC Layers really help all that much.\n",
    "\n",
    "### Drop Out on the Visible Layer\n",
    "Apparently in the original paper, this was recommended.  I've implemented it...seems to work a little, but not much. Requires more epochs, likely because it's equivalent to discarding input data altogether...\n",
    "Some [useful tips](http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/).\n",
    "\n",
    "\n",
    "### Drop Out on the Fully-Connected Layer\n",
    "This can be helpful, however there are definitely some precautions to take --- especially if you don't have a GPU, which was my issue for about a week!  For example, if you're using a global keep_prob of 0.5, and you apply drop out to the transition between the lasty convolutional layer and the first fully-connected layer you will need many, many epochs to see good results.  This is why I re-designed my layer functions to have keep_prob as an input parameter: it seemed necessary to allow the keep_prob to differ between that applied to the convolutional layers and that applied to the fully-connected layers. This gave a good compromise between the benefits of drop out on a FC layer and the run time required to get the validation accuaracy high up. However, it is only a part of the story.\n",
    "\n",
    "What I ultimately found is that the issue (aside from no GPU!) really has to do with that transition layer between the last convolutional layer and first fully-connected layer.  This transition can have a lot of weights, which can already demand quite some epochs in order to properly learn.  A drop out only adds to this, demanding even more epochs achieve desirable results (even with a keep_prob as  relaxed as 0.9).  I found that, timewise, it can be better to forgo dropout at this transition.  If one has two FC layers, then dropout can be applied between these two layers with less burned on the learning task. However, the first observation still applies:  (i) each time drop out is applied at a layer, it's likely one will require more epochs to successfully train the model; (ii) the more weights one drops per drop out, the more epochs one will likely need.  To balance drop out and number of epochs, one can play around with locally assigning different keep_probs to each transition layer, or one might choose a higher \"global\" keep_prob to apply to all layers. \n",
    "\n",
    "\n",
    "That all said, if all you care about it quickly getting the validation accuracy above 0.5, then taking out the second FC layer does dramatically more to improve the model's performance (e.g., it took 7 epochs on my final model w/o 2nd FCL). However, after getting above 0.50, it hangs around there for quite a while (still near 0.57 after 50 epochs).  \n",
    "\n",
    "This is slightly improved by applying drop out at each layer, but it seems like if you want to get higher than 0.60, you really might need the 2nd FCL.\n",
    "\n",
    "UPDATE:  Once my GPU was working on Amazon again, none of this really mattered!  The true lesson learned, then, is that your hardware can be your biggest bottleneck.  Improve your hardware, and you can explore more optimal regions of the parameter space.\n",
    "\n",
    "Another note:  Small batch sizes still apply.  It is amazing how much quicker (in terms of epoch) a batch size of 32 does than, say, a batch size of 128.  Heck, I even found that 32 performs exceptionally better than a batch size of 50 on my final model.\n",
    "\n",
    "### Are Fully-Connected Layers Worth It?\n",
    "My project is already pretty damn late, so even after designing several networks scoring 0.72 to 0.73 on validation and test I wasn't happy.  That's too close to 0.8, and way too close when your project's late anyway!\n",
    "\n",
    "The fully-connected layers bothered me during the various experimental phases I've gone through.  When I tweak a convolutional layer in a dramatic way, usually there is a noticeable effect on validation accuaracy.  But I found you can change quite a lot in the fully-connected portion of the graph (e.g., add or subtract a layer, use drop out or not) and maintain fairly similar results.\n",
    "\n",
    "Take the following design:  \n",
    "\n",
    "<center> 32x32x3 -> 16x16x25 -> 8x8x50 -> 8x8x100 = 6400 -> 50 </center>\n",
    "\n",
    "| Conv Layer | Outputs | Filter Size | Filter Stride | Pool Size | Pool Stride | Activation | Keep Prob |\n",
    "|------------|---------|-------------|---------------|-----------|-------------|------------|-----------|\n",
    "| CL1 | 25  | (5,5) | (1,1) | (2,2) | (2,2) | relu | 0.5 |\n",
    "| CL2 | 50  | (7,7) | (1,1) | (2,2) | (2,2) | relu | 0.5 |\n",
    "| CL3 | 100 | (3,3) | (1,1) | (1,1) | (1,1) | relu | 0.5 |\n",
    "\n",
    "| FC Layer | Outputs | Activation | Keep Prob |\n",
    "|----------|---------|------------|-----------|\n",
    "| FC1      | 50      | relu       | 0.5       |\n",
    "\n",
    "With the FC Layer, 50 epochs on one CIFAR batch w/ sub-batch size of 32 gives a validation accuracy of 0.553, and 0.639 without the layer.  Worse, if you use 10, 20, 30, or 40 outputs, the model gets stuck at a guess (0.105). If you use 100 outputs, you get back up to 0.607.  It is difficult to say if this would perform better or worse than the FCL-free model given more training.  200 outputs gets up to 0.610. 400 outputs, 0.630.\n",
    "\n",
    "Adding a second FC Layer just kills the performance, at least at 50 epochs.  I played around with this convolution design and a bunch of double FC-layer variations, and all reduced validation accuracy quite a bit.  This might be a case where better hardware and more waiting time improves things, but more than 50 epochs on the full CIFAR set gets pretty damn annoying to experiment with.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "==========================================================================================\n",
    "\n",
    "At one point I swear my GPU stopped working... I wait for like 30 - 60 minutes per run.  It's terrible.\n",
    "\n",
    "### Some miscellaneous Nvidia-related files and directories I found:\n",
    "* nvidia-375 directory installed in /lib:  contains modprobe.conf\n",
    "* Tons of other nvidia files in /usr/bin:  nvidia-{bug-report.sh, cuda-mp-control, cuda-mps-server, debugdump, modprobe, persistencd, settings, smi, xconfig}\n",
    "* /usr/include/nvidia-375\n",
    "* /usr/lib/{nvidia, nvidia-367, nvidia-375, nvidia-375-prime}\n",
    "* /usr/lib32/libvdpau_nvidia.so, nvidia-367, nvidia-375} ...\n",
    "* /usr/local/cuda\n",
    "\n",
    "### How to get CPU/GPU info:\n",
    "* CPU Info:  cat /proc/cpuinfo:  Intel Xeon CPU E5-2650 (64-bit, 2.6 GHz)\n",
    "* GPU Info:  lspci -v:  Looks like it's a Nvidia GK104GL [Grid K520]\n",
    "* Apparently Nvidia GPU info is supposed to be held in /proc/driver/nvidia... But /proc/driver has no \"nvidia\" file or directory on my Udacity AMI...  [source](http://askubuntu.com/questions/5417/how-to-get-gpu-info)\n",
    "* Also, to get info about my Nvidia GPU, I should be able to type \"nvidia-smi\" at the command line. Instead I see \"NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\"\n",
    "\n",
    "dpkg -l | grep nvidia <br/>\n",
    "ii  nvidia-367                  375.39-0ubuntu0.16 amd64              Transitional package for nvidia-375 <br/>\n",
    "ii  nvidia-367-dev              375.39-0ubuntu0.16 amd64              Transitional package for nvidia-375-dev <br/>\n",
    "ii  nvidia-375                  375.39-0ubuntu0.16 amd64              NVIDIA binary driver - version 375.39 <br/>\n",
    "ii  nvidia-375-dev              375.39-0ubuntu0.16 amd64              NVIDIA binary Xorg driver development files <br/>\n",
    "ii  nvidia-modprobe             367.48-0ubuntu1    amd64              Load the NVIDIA kernel driver and create device files <br/>\n",
    "ii  nvidia-opencl-icd-367       375.39-0ubuntu0.16 amd64              Transitional package for nvidia-opencl-icd-375 <br/>\n",
    "ii  nvidia-opencl-icd-375       375.39-0ubuntu0.16 amd64              NVIDIA OpenCL ICD <br/>\n",
    "ii  nvidia-prime                0.8.2              amd64              Tools to enable NVIDIA's Prime <br/>\n",
    "ii  nvidia-settings             367.48-0ubuntu1    amd64              Tool for configuring the NVIDIA graphics driver\n",
    "\n",
    "### Uninstalling/Reinstalling Nvidia stuff..\n",
    "Trying to run nvidia-smi led me to some helpful info [here](https://devtalk.nvidia.com/default/topic/1000340/cuda-setup-and-installation/-quot-nvidia-smi-has-failed-because-it-couldn-t-communicate-with-the-nvidia-driver-quot-ubuntu-16-04/) and [here](http://jackmorrison.me/2014/09/11/CUDA-on-AWS.html).\n",
    "\n",
    "\n",
    "I had a similar result to the OP [here](https://devtalk.nvidia.com/default/topic/1000340/cuda-setup-and-installation/-quot-nvidia-smi-has-failed-because-it-couldn-t-communicate-with-the-nvidia-driver-quot-ubuntu-16-04/) when I typed this command:\n",
    "dpkg -l | grep nvidia <br/>\n",
    "ii  nvidia-367                  375.39-0ubuntu0.16 amd64              Transitional package for nvidia-375 <br/>\n",
    "ii  nvidia-367-dev              375.39-0ubuntu0.16 amd64              Transitional package for nvidia-375-dev <br/>\n",
    "ii  nvidia-375                  375.39-0ubuntu0.16 amd64              NVIDIA binary driver - version 375.39 <br/>\n",
    "ii  nvidia-375-dev              375.39-0ubuntu0.16 amd64              NVIDIA binary Xorg driver development files <br/>\n",
    "ii  nvidia-modprobe             367.48-0ubuntu1    amd64              Load the NVIDIA kernel driver and create device files <br/>\n",
    "ii  nvidia-opencl-icd-367       375.39-0ubuntu0.16 amd64              Transitional package for nvidia-opencl-icd-375 <br/>\n",
    "ii  nvidia-opencl-icd-375       375.39-0ubuntu0.16 amd64              NVIDIA OpenCL ICD <br/>\n",
    "ii  nvidia-prime                0.8.2              amd64              Tools to enable NVIDIA's Prime <br/>\n",
    "ii  nvidia-settings             367.48-0ubuntu1    amd64              Tool for configuring the NVIDIA graphics driver\n",
    "\n",
    "The advice given:\n",
    "\"You seem to have a mix of driver components from several different drivers. This is a broken config.  Follow the steps in the [cuda linux install guide](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#handle-uninstallation) to remove every last scrap of NVIDIA software from your machine.  Then stop, and read the above linked guide in its entirety.  Then pick either the runfile install method, or the package manager install method, and follow the instructions.\"\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "If/when I remove all the Nvidia software, here are [some simple steps](http://jackmorrison.me/2014/09/11/CUDA-on-AWS.html) I might try first.\n",
    "\n",
    "I already figured out the GPU, but to follow the directions step-by-step I did it again a different way:\n",
    "1. DL'd ubuntu-drivers:  sudo apt install ubuntu-drivers-common\n",
    "2. sudo ubuntu-drivers devices\n",
    "\n",
    "```{code}\n",
    "== /sys/devices/pci0000:00/0000:00:03.0 ==\n",
    "modalias : pci:v000010DEd0000118Asv000010DEsd00001014bc03sc00i00\n",
    "vendor   : NVIDIA Corporation\n",
    "model    : GK104GL [GRID K520]\n",
    "driver   : xserver-xorg-video-nouveau - distro free builtin\n",
    "driver   : nvidia-340 - distro non-free recommended\n",
    "\n",
    "== cpu-microcode.py ==\n",
    "driver   : intel-microcode - distro non-free\n",
    "```\n",
    "\n",
    "To figure out what drivers are needed, the OS must be known:\n",
    "```{bash}\n",
    "uname -a\n",
    "Linux ip-172-31-69-77 4.4.0-66-generic #87-Ubuntu SMP Fri Mar 3 15:29:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n",
    "\n",
    "cat /etc/*release*\n",
    "DISTRIB_ID=Ubuntu\n",
    "DISTRIB_RELEASE=16.04\n",
    "DISTRIB_CODENAME=xenial\n",
    "DISTRIB_DESCRIPTION=\"Ubuntu 16.04.1 LTS\"\n",
    "NAME=\"Ubuntu\"\n",
    "VERSION=\"16.04.1 LTS (Xenial Xerus)\"\n",
    "ID=ubuntu\n",
    "ID_LIKE=debian\n",
    "PRETTY_NAME=\"Ubuntu 16.04.1 LTS\"\n",
    "VERSION_ID=\"16.04\"\n",
    "HOME_URL=\"http://www.ubuntu.com/\"\n",
    "SUPPORT_URL=\"http://help.ubuntu.com/\"\n",
    "BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\n",
    "VERSION_CODENAME=xenial\n",
    "UBUNTU_CODENAME=xenial\n",
    "```\n",
    "\n",
    "I think all that corresponds to these drivers: <br/>\n",
    "http://www.nvidia.com/download/driverResults.aspx/108586/en-us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
